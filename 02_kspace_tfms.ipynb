{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"02_kspace_tfms.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github","colab_type":"text"},"source":["<a href=\"https://colab.research.google.com/github/RGologorsky/fastmri/blob/master/01_kspace_tfms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"GhcmjtAuMFPH","colab_type":"text"},"source":["## Kspace tfms\n","\n","- Implemented kspace transforms with np and Pytorch tensors\n","- Testing in testing_01_kspace_tfms.ipynb."]},{"cell_type":"markdown","metadata":{"id":"pQyNnL5n5aov","colab_type":"text"},"source":["For real images, \n","- FFT of a purely real array (eg image) has conjugate symmetry: ```\n","fft2_result[a, b] = fft2_result[-a, -b].conj()\n","```\n","- So rfft2 ouputs the left half (plus one column) of fft2 to save space & memory\n","\n","- RFFT output could correspond to either an odd or even length signal.\n","- By default, irfft assumes an even output length.\n","- To avoid losing information, when doing inverse (irfft), given the correct length of the original input.\n","\n","Sources: \n","\n","- https://stackoverflow.com/questions/43001729/how-should-i-interpret-the-output-of-numpy-fft-rfft2\n","\n","- https://numpy.org/doc/stable/reference/generated/numpy.fft.irfft.html"]},{"cell_type":"code","metadata":{"id":"bTME9Og7Q7VU","colab_type":"code","colab":{}},"source":["import scipy.fft as SP\n","import numpy.fft as NP"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NuaLeu7z7BB9","colab_type":"code","colab":{}},"source":["class NpTfms():\n","\n","  # rfft w/ normalization\n","  rfft2  = partial(SP.rfft2,  norm=\"ortho\")\n","  irfft2 = partial(SP.irfft2, norm=\"ortho\")\n","\n","  # fft w/ normalization - does not require complex input\n","  fft2  = partial(SP.fft2,  norm=\"ortho\")\n","  ifft2 = partial(SP.ifft2, norm=\"ortho\")\n","\n","\n","  # convert image to np array\n","  im2arr   = [np.array]\n","\n","  # (uncentered) real arr obj to/from (centered) kspace\n","  real2k  = [rfft2, NP.fftshift]\n","  \n","  @classmethod\n","  def k2real(cls, s=None): return [NP.ifftshift, partial(cls.irfft2, s=s)]\n","\n","  # same as above, but fft instead of rfft\n","  fft_real2k  = [fft2, NP.fftshift]\n","  fft_k2real  = [NP.ifftshift, ifft2]\n","  \n","  # (centered) complex arr obj to/from (centered) kspace\n","  complex2k  = [NP.ifftshift, fft2, NP.fftshift]\n","  k2complex  = [NP.ifftshift, ifft2, NP.fftshift]\n","   \n","  # Viz: kspace to amplitude-only img (log scale)\n","  np_abs = [np.abs]\n","  log_abs = [add(1e-9), np.log, np.abs]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qNGGhx_Y_L_b","colab_type":"code","outputId":"d33732f8-660a-4371-fd51-78012d2aa97d","colab":{"base_uri":"https://localhost:8080/","height":350}},"source":["class TensorTfms():\n","\n","  # rfft in 2dim\n","  rfft2  = partial(torch.rfft,  signal_ndim = 2, normalized=True)\n","  irfft2 = partial(torch.irfft, signal_ndim = 2, normalized=True)\n","\n","  # fft in 2dim - expects complex input\n","  fft2  = partial(torch.fft,  signal_ndim = 2, normalized=True)\n","  ifft2 = partial(torch.ifft, signal_ndim = 2, normalized=True)\n","\n","  # batch fft & ifft shift -- shift all but batch dimension\n","  def batch_ifftshift(x):\n","    dim = tuple(range(x.dim()))[1:]\n","    shift = [(dim + 1) // 2 for dim in x.shape[1:]]\n","    return T.roll(x, shift, dim)\n","\n","  def batch_fftshift(x):\n","    dim = tuple(range(x.dim()))[1:]\n","    shift = [dim // 2 for dim in x.shape[1:]]\n","    return T.roll(x, shift, dim)\n","\n","  # convert image to Pytorch tensor\n","  def im2arr(im): return tensor(im).double()\n","\n","  # convert real tensor into complex tensor\n","  def real2complex(t): return torch.stack((t, torch.zeros(t.shape).double()), axis=-1)\n","\n","  # (uncentered) real arr obj to/from (centered) kspace\n","  \n","  #real2k  = [rfft2, T.fftshift]\n","  @classmethod\n","  def real2k(cls, onesided=True): \n","    return [partial(cls.rfft2, onesided=onesided), T.fftshift]\n","\n","\n","  @classmethod\n","  def k2real(cls, s=None, onesided=True): \n","    return [T.ifftshift, partial(cls.irfft2, signal_sizes=s, onesided=onesided)]\n","\n"," # BATCH version -- does not shift the batch axis \n","  @classmethod\n","  def batch_real2k(cls, onesided=True): \n","    return [partial(cls.rfft2, onesided=onesided), cls.batch_fftshift]\n","\n","  # BATCH version -- does not shift the batch axis \n","  @classmethod\n","  def batch_k2real(cls, s=None, onesided=True): \n","    return [cls.batch_ifftshift, partial(cls.irfft2, signal_sizes=s, onesided=onesided)]\n","\n","  # same as above, but fft instead of rfft\n","  fft_real2k  = [fft2, T.fftshift]\n","  fft_k2real  = [T.ifftshift, ifft2, T.complex_abs]\n","  \n","  # (centered) complex arr obj to/from (centered) kspace\n","  complex2k  = [T.fft2]\n","  k2complex  = [T.to_tensor, T.ifft2]\n","\n","  # BATCH version -- does not shift the batch axis \n","  batch_fft_real2k = [fft2, batch_fftshift] \n","  batch_fft_k2real = [batch_ifftshift, ifft2, T.complex_abs]\n","   \n","  # Viz: kspace to amplitude-only img (log scale)\n","  t_abs = [T.complex_abs]\n","  log_abs = [add(1e-9), torch.log, torch.abs]"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-afa842ac1569>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTensorTfms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;31m# rfft in 2dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mrfft2\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrfft\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0msignal_ndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mirfft2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mirfft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignal_ndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-afa842ac1569>\u001b[0m in \u001b[0;36mTensorTfms\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;31m# rfft in 2dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mrfft2\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrfft\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0msignal_ndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mirfft2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mirfft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignal_ndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'partial' is not defined"]}]},{"cell_type":"code","metadata":{"id":"FtWdpSSOc4nO","colab_type":"code","colab":{}},"source":["class TensorTfmsBase():\n","\n","  # convert image to Pytorch tensor, real tensor into complex tensor\n","  def im2arr(im):      return tensor(im).double()\n","  def real2complex(t): return torch.stack((t, torch.zeros(t.shape).double()), axis=-1)\n","\n","  # real to centered kspace (and back)\n","  @classmethod def real2k(cls): return [cls.fft, T.fftshift]\n","  @classmethod def k2real(cls): return [T.ifftshift, cls.ifft]\n","  \n","  # batch methods do not shift batch axis\n","  @classmethod def batch_real2k(cls): return [cls.fft, batch_ffshift] \n","  @classmethod def batch_k2real(cls): return [batch_ifftshift, self.ifft]\n","\n","  # (centered) complex arr obj to (centered) kspace (and back)\n","  complex2k  = [T.fft2]\n","  k2complex  = [T.to_tensor, T.ifft2]\n","   \n","  # Viz complex k in magnitude-only kspace (log)\n","  magn = [T.complex_abs]\n","  log_abs = [add(1e-9), torch.log, torch.abs]\n","\n","\n","class TensorTfmsReal(TensorTfmsBase):\n","  @classmethod \n","  def fft(onesided=True): \n","    return partial(torch.rfft,  signal_ndim = 2, normalized=True, onesided=onesided)\n"," \n","  @classmethod \n","  def ifft(s=None, onesided=True): \n","    return partial(torch.rfft,  signal_ndim = 2, normalized=True, signal_sizes=s, onesided=onesided)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vpzTQ4TzbS46","colab_type":"code","colab":{}},"source":["def apply(data, tfms, pre=None, post=None): return Pipeline(L(pre) + L(tfms) + L(post))(data)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FyIbaRXhhWQT","colab_type":"text"},"source":["## Viz: function to plot images"]},{"cell_type":"code","metadata":{"id":"UlD5FyNoha2T","colab_type":"code","colab":{}},"source":["def idx(lst,i, default=None): return lst[i] if i < len(lst) else default"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YQ5MxTTMYWrZ","colab_type":"code","colab":{}},"source":["def plot(imgs, titles=[None], cmaps=[\"gray\"], nrows=1, ncols=1, figsize = (6,6), **kwargs):\n","\n","   # listify so we input can be string instead of 1-item list\n","  imgs, titles, cmaps = L(imgs), L(titles), L(cmaps)\n","\n","  # default set nrows, ncols = 1, len(imgs)\n","  if nrows * ncols != len(imgs): nrows, ncols = 1, len(imgs)\n","\n","  # default repeat cmap until same size as images\n","  cmaps = cmaps * int(len(imgs)/len(cmaps))\n","\n","  fig,axes = plt.subplots(nrows, ncols, figsize=figsize, squeeze=False)\n","  axes = axes.flatten()\n","  for i,im in enumerate(imgs): \n","    axes[i].imshow(im, cmap=idx(cmaps,i))\n","    axes[i].set_xticklabels([]), axes[i].set_yticklabels([])\n","    axes[i].set_title(idx(titles,i))\n","    axes[i].set_xlabel(im.shape)\n","  fig.tight_layout()\n","  fig.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HZQCqUPw3q9n","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zWqdogDG3njV","colab_type":"text"},"source":["# Common KSpace Transforms"]},{"cell_type":"code","metadata":{"id":"QbRrs6P-ku2d","colab_type":"code","colab":{}},"source":["# permute kspace tensor: N1HW(Complex) to N(Complex)HW\n","class Complex2Channel(Transform):\n","  order = 99 # happens after complex k\n","\n","  # N1HW(Complex) -> N(Complex)HW\n","  def encodes(self, t:Tensor):\n","    if t.size(-1) == 2: return torch.squeeze(t.transpose(-1,-2).transpose(-2,-3))\n","    return t\n","\n","  # NCHW -> NHWC\n","  def decodes(self, t:Tensor):\n","    if t.size(-3) == 2: return t.transpose(-3,-2).transpose(-2,-1)\n","    return t\n","    \n","# shows concat real & kspace into one image\n","class ShowK(Tuple):\n","  def show(self, ctx=None, **kwargs): \n","    k,real = self\n","    line = k.new_zeros(k.shape[0], 10)\n","    return show_image(torch.cat([k,line,real], dim=1), title = \"K & Real\", ctx=ctx, **kwargs)\n","\n","# take dataset item (real img, category), convert to (k arr, category)\n","class BatchReal2ComplexK(Transform):\n","  order = 50 # needs to run after save shape\n","\n","  # do nothing to tensor categories\n","  def encodes(self, t:TensorCategory): return t\n","  def decodes(self, t:TensorCategory): return t\n","\n","  def encodes(self, t:Tensor): return apply(t, TensorTfms.batch_real2k(onesided=False))\n","\n","\n","  def decodes(self, t_k:Tensor):\n","    t_k_abs         = apply(t_k, TensorTfms.t_abs)\n","    t_k_log_abs     = apply(t_k_abs, TensorTfms.log_abs)\n","\n","    t_real     = apply(t_k, TensorTfms.batch_k2real(onesided=False))\n","    \n","    return ShowK(t_k_log_abs, t_real)\n","\n","# converts complex k-space (2channel) to amplitude k-space (1channel)\n","class ComplexK2LogAbs(Transform):\n","  order = 51 # needs to run after Real2ComplexK\n","\n","  # do nothing to tensor categories\n","  def encodes(self, t:TensorCategory): return t\n","  def decodes(self, t:TensorCategory): return t\n","\n","  def encodes(self, t:Tensor): return apply(t, TensorTfms.log_abs, pre=TensorTfms.t_abs)"],"execution_count":0,"outputs":[]}]}