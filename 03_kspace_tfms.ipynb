{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03_kspace_tfms.ipynb","provenance":[{"file_id":"1Sa_ebRVKC9ywRnLrmIHlQGiP7pSjt-LC","timestamp":1591986585689}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github","colab_type":"text"},"source":["<a href=\"https://colab.research.google.com/github/RGologorsky/fastmri/blob/master/01_kspace_tfms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"GhcmjtAuMFPH","colab_type":"text"},"source":["## Kspace tfms\n","\n","- Implemented kspace transforms with np and Pytorch tensors\n","- Testing in testing_01_kspace_tfms.ipynb."]},{"cell_type":"markdown","metadata":{"id":"pQyNnL5n5aov","colab_type":"text"},"source":["For real images, \n","- FFT of a purely real array (eg image) has conjugate symmetry: ```\n","fft2_result[a, b] = fft2_result[-a, -b].conj()\n","```\n","- So rfft2 ouputs the left half (plus one column) of fft2 to save space & memory\n","\n","- RFFT output could correspond to either an odd or even length signal.\n","- By default, irfft assumes an even output length.\n","- To avoid losing information, when doing inverse (irfft), given the correct length of the original input.\n","\n","Sources: \n","\n","- https://stackoverflow.com/questions/43001729/how-should-i-interpret-the-output-of-numpy-fft-rfft2\n","\n","- https://numpy.org/doc/stable/reference/generated/numpy.fft.irfft.html"]},{"cell_type":"code","metadata":{"id":"FtWdpSSOc4nO","colab_type":"code","colab":{}},"source":["class TensorTfmsBase():\n","\n","  # convert image to Pytorch tensor, real tensor into complex tensor\n","  @staticmethod\n","  def im2arr(im):      return tensor(im).double()\n","  \n","  @staticmethod\n","  def real2complex(t): return torch.stack((t, torch.zeros(t.shape).double()), axis=-1)\n","\n","  # (centered) complex arr obj to (centered) kspace (and back)\n","  complex2k  = [T.fft2]\n","  k2complex  = [T.to_tensor, T.ifft2]\n","   \n","  # Viz complex k in magnitude-only kspace (log)\n","  log_magn = [T.complex_abs, add(1e-9), torch.log, torch.abs]\n","\n","  # FFT & Shifting\n","  fftshift  = T.fftshift\n","  ifftshift = T.ifftshift\n","\n","  # real to centered kspace (and back)\n","  @classmethod\n","  def real2k(cls): return [cls.fft, cls.fftshift]\n","  @classmethod\n","  def k2real(cls): return [cls.ifftshift, cls.ifft]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kY8xd3f8Cr8P","colab_type":"code","colab":{}},"source":["class TensorTfms(TensorTfmsBase):\n"," fft = T.fft2\n","\n"," # ifft = ifft2 + magnitude\n"," def ifft(x): return T.complex_abs(T.ifft2(x))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FYC6GUd6CsoY","colab_type":"code","colab":{}},"source":["class RealTensorTfms(TensorTfmsBase):\n","   # real to centered kspace (and back)\n","  @classmethod\n","  def real2k(cls, onesided=True):\n","     cls.fft = partial(torch.rfft, signal_ndim = 2, normalized=True, onesided=onesided)\n","     return super().real2k()\n","\n","  @classmethod\n","  def k2real(cls, onesided=True, s=None): \n","    cls.ifft =  partial(torch.irfft, signal_ndim = 2, normalized=True, onesided=onesided, signal_sizes=s)\n","    return super().k2real()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i8ueVDE0Cq6U","colab_type":"code","colab":{}},"source":["class BatchShift():\n","  # batch fft & ifft shift -- shift all but batch dimension\n","  def ifftshift(x):\n","    dim = tuple(range(x.dim()))[1:]\n","    shift = [(dim + 1) // 2 for dim in x.shape[1:]]\n","    return T.roll(x, shift, dim)\n","\n","  def fftshift(x):\n","    dim = tuple(range(x.dim()))[1:]\n","    shift = [dim // 2 for dim in x.shape[1:]]\n","    return T.roll(x, shift, dim)\n","\n","class BatchTensorTfms(TensorTfms, BatchShift): pass\n","class BatchRealTensorTfms(RealTensorTfms, BatchShift): pass"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vpzTQ4TzbS46","colab_type":"code","colab":{}},"source":["def apply(data, tfms, pre=None, post=None): return Pipeline(L(pre) + L(tfms) + L(post))(data)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FyIbaRXhhWQT","colab_type":"text"},"source":["## Viz: function to plot images"]},{"cell_type":"code","metadata":{"id":"UlD5FyNoha2T","colab_type":"code","colab":{}},"source":["def idx(lst,i, default=None): return lst[i] if i < len(lst) else default"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YQ5MxTTMYWrZ","colab_type":"code","colab":{}},"source":["def plot(imgs, titles=[None], cmaps=[\"gray\"], nrows=1, ncols=1, figsize = (6,6), **kwargs):\n","\n","   # listify so we input can be string instead of 1-item list\n","  imgs, titles, cmaps = L(imgs), L(titles), L(cmaps)\n","\n","  # default set nrows, ncols = 1, len(imgs)\n","  if nrows * ncols != len(imgs): nrows, ncols = 1, len(imgs)\n","\n","  # default repeat cmap until same size as images\n","  cmaps = cmaps * int(len(imgs)/len(cmaps))\n","\n","  fig,axes = plt.subplots(nrows, ncols, figsize=figsize, squeeze=False)\n","  axes = axes.flatten()\n","  for i,im in enumerate(imgs): \n","    axes[i].imshow(im, cmap=idx(cmaps,i))\n","    axes[i].set_xticklabels([]), axes[i].set_yticklabels([])\n","    axes[i].set_title(idx(titles,i))\n","    axes[i].set_xlabel(im.shape)\n","  fig.tight_layout()\n","  fig.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HZQCqUPw3q9n","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zWqdogDG3njV","colab_type":"text"},"source":["# Common KSpace Transforms"]},{"cell_type":"code","metadata":{"id":"QbRrs6P-ku2d","colab_type":"code","colab":{}},"source":["# permute kspace tensor: N1HW(Complex) to N(Complex)HW\n","class Complex2Channel(Transform):\n","  order = 99 # happens after complex k\n","\n","  # N1HW(Complex) -> N(Complex)HW\n","  def encodes(self, t:Tensor):\n","    if t.size(-1) == 2: return torch.squeeze(t.transpose(-1,-2).transpose(-2,-3))\n","    return t\n","\n","  # NCHW -> NHWC\n","  def decodes(self, t:Tensor):\n","    if t.size(-3) == 2: return t.transpose(-3,-2).transpose(-2,-1)\n","    return t\n","    \n","# shows concat real & kspace into one image\n","class ShowK(Tuple):\n","  def show(self, ctx=None, **kwargs): \n","    k,real = self\n","    line = k.new_zeros(k.shape[0], 10)\n","    return show_image(torch.cat([k,line,real], dim=1), title = \"K & Real\", ctx=ctx, **kwargs)\n","\n","# take dataset item (real img, category), convert to (k arr, category)\n","class BatchReal2ComplexK(Transform):\n","  order = 50 # needs to run after save shape\n","\n","  # do nothing to tensor categories\n","  def encodes(self, t:TensorCategory): return t\n","  def decodes(self, t:TensorCategory): return t\n","\n","  def encodes(self, t:Tensor): return apply(t, TensorTfms.batch_real2k(onesided=False))\n","\n","\n","  def decodes(self, t_k:Tensor):\n","    t_k_abs         = apply(t_k, TensorTfms.t_abs)\n","    t_k_log_abs     = apply(t_k_abs, TensorTfms.log_abs)\n","\n","    t_real     = apply(t_k, TensorTfms.batch_k2real(onesided=False))\n","    \n","    return ShowK(t_k_log_abs, t_real)\n","\n","# converts complex k-space (2channel) to amplitude k-space (1channel)\n","class ComplexK2LogAbs(Transform):\n","  order = 51 # needs to run after Real2ComplexK\n","\n","  # do nothing to tensor categories\n","  def encodes(self, t:TensorCategory): return t\n","  def decodes(self, t:TensorCategory): return t\n","\n","  def encodes(self, t:Tensor): return apply(t, TensorTfms.log_abs, pre=TensorTfms.t_abs)"],"execution_count":0,"outputs":[]}]}